apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: his-custom-alert-rules
    role: alert-rules
  name: kubernetes-service-custom-rule
  namespace: monitoring
spec:
  groups:
  - name: kubernetes-service-custom-rule:kubernetes-alert
    rules:
    - alert: KubernetesContainerOomKiller
      annotations:
        current_value: '{{ .Value }}'
        description: Container {{ $labels.container }} in pod {{ $labels.namespace }} {{ $labels.pod }} has been OOMKilled times in the last 10 minutes
        summary: |
          Kubernetes container oom killer instance {{ $labels.instance }} {{ $labels.pod }}
      expr: ((kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total
        offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]))>0
      for: 1s
      labels:
        alert_name: KubernetesContainerOomKiller
        alert_type: metric
        cluster_name: 'prod-kim'
        comparison: greater than
        duration: 1s
        expression: ((kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total
          offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]))>0
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesContainerOomKiller
        severity: warning
        threshold_value: "0"
    - alert: KubernetesStatefulsetDown
      annotations:
        current_value: '{{ .Value }}'
        description: A StatefulSet {{ $labels.statefulset }} went down
        summary: |
          Kubernetes StatefulSet {{ $labels.statefulset }} down instance {{ $labels.instance }}
      expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current)!=1
      for: 1m
      labels:
        alert_name: KubernetesStatefulsetDown
        alert_type: metric
        cluster_name: 'prod-kim'
        comparison: not equal
        duration: 1m
        expression: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current)!=1
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesStatefulsetDown
        severity: warning
        threshold_value: "1"
    - alert: KubernetesPodNotHealthy
      annotations:
        current_value: '{{ .Value }}'
        description: Pod {{ $labels.pod }} has been in a non-ready state for longer than 15 minutes
        summary: |
          Kubernetes Pod not healthy instance {{ $labels.instance }}
      expr: sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}) > 0
      for: 15m
      labels:
        alert_name: KubernetesPodNotHealthy
        alert_type: metric
        cluster_name: 'prod-kim'
        comparison: greater than
        duration: 15m
        expression: sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}) > 0
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesPodNotHealthy
        severity: warning
        threshold_value: "0"
    - alert: KubernetesPodCrashLooping
      annotations:
        current_value: '{{ .Value }}'
        description: Pod {{ $labels.pod }} is crash looping
        summary: |
          Kubernetes pod crash looping instance {{ $labels.instance }}
      expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
      for: 2m
      labels:
        alert_name: KubernetesPodCrashLooping
        alert_type: metric
        cluster_name: 'prod-kim'
        comparison: greater than
        duration: 2m
        expression: rate(kube_pod_increase(kube_pod_container_status_restarts_total[1m]) > 3
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesPodCrashLooping
        severity: warning
        threshold_value: "1"
    - alert: KubernetesJobFailed
      annotations:
        current_value: '{{ .Value }}'
        description: Job {{$labels.namespace}} {{$labels.exported_job}} failed to complete
        summary: |
          Kubernetes Job failed on instance {{ $labels.host_ip }} {{ $labels.job }} {{ $labels.job_name }}
      expr: kube_job_status_failed > 0
      for: 1m
      labels:
        alert_name: KubernetesJobFailed
        alert_type: metric
        cluster_name: 'prod-kim'
        comparison: greater than
        duration: 1m
        expression: kube_job_status_failed > 0
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesJobFailed
        severity: warning
        threshold_value: "1"
    - alert: KubernetesPVCPending
      annotations:
        current_value: '{{ .Value }}'
        description: Kubernetes PVC {{ $labels.persistentvolumeclaim }} on {{ $labels.namespace }} is pending
        summary: |
          Kubernetes PVC pending instance {{ $labels.instance }}
      expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
      for: 2m
      labels:
        alert_name: KubernetesPVCPending
        alert_type: metric
        cluster_name: 'prod-kim'
        duration: 2m
        expression: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesPVCPending
        severity: warning
        threshold_value: "1"
    - alert: KubernetesVolumeFullIn4Days
      annotations:
        current_value: '{{ .Value }}'
        description: Kubernetes Volume {{ $labels.persistentvolumeclaim }} on {{ $labels.namespace }} is expected to fill up within 4 days. Currently {{ $value }}B
        summary: |
          Kubernetes Volume full in 4 days instance {{ $labels.instance }}
      expr: (predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600)) < 0
      for: 1m
      labels:
        alert_name: KubernetesVolumeFullIn4Days
        alert_type: metric
        cluster_name: 'prod-kim'
        duration: 1m
        expression: (predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600)) < 0
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesVolumeFullIn4Days
        severity: warning
        threshold_value: "1"
    - alert: KubernetesPersistentVolumeError
      annotations:
        current_value: '{{ .Value }}'
        description: Kubernetes Persistent volume {{ $labels.persistentvolume }}is in bad state
        summary: |
          Kubernetes Persistent volume error instance {{ $labels.instance }} {{ $labels.namespace }}
      expr: (kube_persistentvolume_status_phase{phase=~"Failed|Pending"}) > 0
      for: 1m
      labels:
        alert_name: KubernetesPersistentVolumeError
        alert_type: metric
        cluster_name: 'prod-kim'
        duration: 1m
        expression: (kube_persistentvolume_status_phase{phase=~"Failed|Pending"}) > 0
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesPersistentVolumeError
        severity: warning
        threshold_value: "1"
    - alert: KubernetesStatefulsetReplicasMismatch
      annotations:
        current_value: '{{ .Value }}'
        description: A StatefulSet does not match the expected number of replicas {{ $labels.replicaset }}
        summary: |
          Kubernetes StatefulSet replicas mismatch instance {{ $labels.instance }} {{ $labels.replicaset }}  {{ $labels.namespace }}
      expr: (kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas)
      for: 5m
      labels:
        alert_name: KubernetesStatefulsetReplicasMismatch
        alert_type: metric
        cluster_name: 'prod-kim'
        duration: 5m
        expression: (kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas)
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesStatefulsetReplicasMismatch
        severity: warning
        threshold_value: "1"
    - alert: KubernetesDeploymentReplicasMismatch
      annotations:
        current_value: '{{ .Value }}'
        description: Deployment Replicas mismatch {{ $labels.replicaset }}
        summary: |
          Kubernetes Deployment replicas mismatch instance {{ $labels.instance }} {{ $labels.replicaset }} {{ $labels.namespace }}
      expr: (kube_deployment_spec_replicas != kube_deployment_status_replicas_available)
      for: 5m
      labels:
        alert_name: KubernetesDeploymentReplicasMismatch
        alert_type: metric
        cluster_name: 'prod-kim'
        duration: 5m
        expression: (kube_deployment_spec_replicas != kube_deployment_status_replicas_available)
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesDeploymentReplicasMismatch
        severity: warning
        threshold_value: "1"
    - alert: KubernetesNodeReady
      annotations:
          description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          summary: Kubernetes Node ready (instance {{ $labels.instance }})
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 10m
      labels:
        alert_name: KubernetesNodeReady
        alert_type: metric
        duration: 10m
        cluster_name: 'prod-kim'
        expression: kube_node_status_condition{condition="Ready",status="true"} == 0
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesNodeReady
        severity: critical
    - alert: KubernetesDiskPressure
      annotations:
          description: "{{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          summary: Kubernetes disk pressure (instance {{ $labels.instance }})
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 5m
      labels:
        alert_name: KubernetesDiskPressure
        alert_type: metric
        duration: 5m
        cluster_name: 'prod-kim'
        expression: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesDiskPressure
        severity: critical
    - alert: KubernetesNetworkUnavailable
      annotations:
          description: "{{ $labels.node }} has NetworkUnavailable condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          summary: Kubernetes network unavailable (instance {{ $labels.instance }})
      expr: kube_node_status_condition{condition="NetworkUnavailable",status="true"} == 1
      for: 5m
      labels:
        alert_name: KubernetesNetworkUnavailable
        alert_type: metric
        duration: 5m
        cluster_name: 'prod-kim'
        expression: kube_node_status_condition{condition="NetworkUnavailable",status="true"} == 1
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesNetworkUnavailable
        severity: critical
    - alert: KubernetesOutOfCapacity
      annotations:
          description: "{{ $labels.node }} is out of capacity\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          summary: Kubernetes out of capacity (instance {{ $labels.instance }})
      expr: sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=""})) / sum by (node) (kube_node_status_allocatable{resource="pods"}) * 100 > 90
      for: 5m
      labels:
        alert_name: KubernetesOutOfCapacity
        alert_type: metric
        duration: 5m
        cluster_name: 'prod-kim'
        expression: sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=""})) / sum by (node) (kube_node_status_allocatable{resource="pods"}) * 100 > 90
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesOutOfCapacity
        severity: critical
    - alert: KubernetesVolumeOutOfDiskSpace
      annotations:
          description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          summary: Kubernetes Volume out of disk space (instance {{ $labels.instance }})
      expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
      for: 5m
      labels:
        alert_name: KubernetesVolumeOutOfDiskSpace
        alert_type: metric
        duration: 5m
        cluster_name: 'prod-kim'
        expression: kube_node_status_condition{condition="NetworkUnavailable",status="true"} == 1
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesVolumeOutOfDiskSpace
        severity: critical
    - alert: KubernetesHpaScalingAbility
      annotations:
          description: "Pod is unable to scale\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          summary: Kubernetes HPA scaling ability (instance {{ $labels.instance }})
      expr: kube_horizontalpodautoscaler_status_condition{status="false", condition="AbleToScale"} == 1
      for: 5m
      labels:
        alert_name: KubernetesHpaScalingAbility
        alert_type: metric
        duration: 5m
        cluster_name: 'prod-kim'
        expression: kube_horizontalpodautoscaler_status_condition{status="false", condition="AbleToScale"} == 1
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_KubernetesHpaScalingAbility
        severity: critical
    - alert: WorkerNodeHighCPU
      expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{container="node-exporter",mode="idle"}[2m])) * 100) > 90
      for: 10m
      labels:
        severity: "critical"
      annotations:
        summary: "High usage CPU in worker node - IP: {{ $labels.instance }} in 15mins"
        description: "CPU load is > 90% on worker node - IP: {{ $labels.instance }} in 15mins"
    - alert: ContainerAbsent
      expr: absent(container_last_seen{name="nodeexporter"}) or absent(container_last_seen{name="mysql_exporter"}) or absent(container_last_seen{name="redis"}) or absent(container_last_seen{name="rabbitmq"}) or absent(container_last_seen{name="graylog_graylog_1"}) or absent(container_last_seen{name="graylog_elasticsearch_1"}) or absent(container_last_seen{name="graylog_mongo_1"})
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Container not running in dc-his-prodsvc IP: 10.96.17.149"
        description: "A container {{ $labels.name }} is absent for 2 min"
    #- alert: WorkerNodeHighMemory
    #  expr: (sum by (instance, job) (node_memory_MemTotal_bytes{job="node-exporter"}) - sum by (instance, job) (node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Buffers_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"}) ) / sum by (instance, job) (node_memory_MemTotal_bytes{job="node-exporter"}) * 100 > 90
    #  for: 15m
    #  labels:
    #    severity: "critical"
    #  annotations:
    #    summary: "Worker node - IP: {{ $labels.instance }} memory is almost full in 15mins"
    #    description: "Docker host memory usage is {{ humanize $value}}%. Reported by worker node - IP: {{ $labels.instance }} in 15mins"
##Blackbox exporter
    - alert: ProbeFailed
      annotations:
          description: "Blackbox probe failed (instance {{ $labels.instance }})"
          summary: "Probe failed\n  VALUE = {{ $value }}"
      expr: probe_success == 0
      for: 1m
      labels:
        alert_name: ProbeFailed
        alert_type: metric
        duration: 1m
        cluster_name: 'prod-kim'
        expression: probe_success == 0
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_ProbeFailed
        severity: critical
    - alert: ProbeHttpFailure
      annotations:
          description: "Curl to (instance {{ $labels.instance }}) failed"
          summary: "HTTP status code is not 200\n  VALUE = {{ $value }}"
      expr: probe_http_status_code != 200
      for: 1m
      labels:
        alert_name: ProbeHttpFailure
        alert_type: metric
        duration: 1m
        cluster_name: 'prod-kim'
        expression: probe_http_status_code != 200
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_ProbeHttpFailure
        severity: critical
    - alert: SslCertificateWillExpireSoon20Days
      annotations:
          description: "SSL certificate will expire soon (instance {{ $labels.instance }})"
          summary: "SSL certificate expires in less than 20 days\n  VALUE = {{ $value }}"
      expr: 3 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 20
      for: 0m
      labels:
        alert_name: SslCertificateWillExpireSoon20Days
        alert_type: metric
        duration: 0m
        cluster_name: 'prod-kim'
        expression: 3 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 20
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_SslCertificateWillExpireSoon20Days
        severity: warning
    - alert: SslCertificateWillExpireSoon5Days
      annotations:
          description: "SSL certificate will expire soon (instance {{ $labels.instance }})"
          summary: "SSL certificate expires in less than 5 days\n  VALUE = {{ $value }}"
      expr: 0 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 5
      for: 0m
      labels:
        alert_name: SslCertificateWillExpireSoon5Days
        alert_type: metric
        duration: 0m
        cluster_name: 'prod-kim'
        expression: 0 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 5
        group_id: kubernetes-service-custom-rule:kubernetes-alert
        rule_id: kubernetes-service-custom-rule:kubernetes-alert_SslCertificateWillExpireSoon5Days
        severity: warning
###Node DSVC
    - alert: HostDSVCHighMemoryUsage
      expr: (sum by (instance, job, ip) (node_memory_MemTotal_bytes{job="node_exporter_kim"}) - sum by (instance, job, ip) (node_memory_MemFree_bytes{job="node_exporter_kim"} + node_memory_Buffers_bytes{job="node_exporter_kim"} + node_memory_Cached_bytes{job="node_exporter_kim"}) ) / sum by (instance, job, ip) (node_memory_MemTotal_bytes{job="node_exporter_kim"}) * 100 > 90
      for: 2m
      labels:
        severity: "warning"
      annotations:
        summary: "Server memory is almost full"
        description: "Docker host memory usage is {{ humanize $value}}%. Reported by instance {{ $labels.instance }} {{ $labels.ip }} of job {{ $labels.job }}."
    - alert: HostDSVCHighCpuLoad
      expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{job="node_exporter_kim",mode="idle"}[2m])) * 100) > 90
      for: 0m
      labels:
        severity: "warning"
      annotations:
        summary: "Docker host high CPU load (instance {{ $labels.instance }} {{ $labels.ip }})"
        description: "CPU load is > 80% on instance {{ $labels.instance }} {{ $labels.ip }}"
    - alert: HostDSVCOutOfDisk
      expr:  ((node_filesystem_avail_bytes{job="node_exporter_kim",mountpoint=~"/"} / 1024 / 1024 / 1024) < 20) and ((100 - (node_filesystem_avail_bytes{job="node_exporter_kim",mountpoint=~"/"} * 100) / node_filesystem_size_bytes{job="node_exporter_kim",mountpoint=~"/"}) > 85)
      for: 5m
      labels:
        severity: "critical"
      annotations:
        summary: "Docker host out of disk space less than 20GB and disk usage root more than 85% on {{ $labels.instance }} {{ $labels.ip }} "
        description: "Docker host out of disk space less than 20GB and disk usage root more than 85% on {{ $labels.instance }} {{ $labels.ip }}"
    - alert: MinIONodesOffline
      expr: avg_over_time(minio_cluster_nodes_offline_total{job="minio-job"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Node down in MinIO deployment"
        description: "Node(s) in cluster {{ $labels.instance }} offline for more than 5 minutes"
    - alert: MinIODisksOffline
      expr: avg_over_time(minio_cluster_disk_offline_total{job="minio-job"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Disks down in MinIO deployment"
        description: "Disks(s) in cluster {{ $labels.instance }} offline for more than 5 minutes"
##HPA
    - alert: DeploymentScaleOverFourReplicas
      expr: kube_horizontalpodautoscaler_status_desired_replicas > 3
      for: 1m
      labels:
        severity: warning
      annotations:
          description: "Deployment {{ $labels.horizontalpodautoscaler }} scale over 3 replicas"
          summary: "HPA Scale over 3 pods \n  VALUE = {{ $value }}"
##MYSQL
    - alert: MysqlDown
      expr: mysql_up == 0
      for: 1m
      labels:
        group: HISMySQLAlerts
      annotations:
        summary: MySQL down (instance {{ $labels.instance }})
        description: "MySQL instance is down on {{ $labels.instance }} {{ $labels.ip }}"
    - alert: MysqlRestarted
      expr: mysql_global_status_uptime < 60
      for: 1m
      labels:
        group: HISMySQLAlerts
      annotations:
        summary: MySQL restarted (instance {{ $labels.instance }})
        description: "MySQL has just been restarted, less than one minute ago on {{ $labels.instance }} {{ $labels.ip }}"
    - alert: MysqlSlowQueries
      expr: increase(mysql_global_status_slow_queries[1m]) > 0
      for: 2m
      labels:
        group: HISMySQLAlerts
      annotations:
        summary: MySQL slow queries > 60s (instance {{ $labels.instance }} )
        description: "MySQL server {{ $labels.instance }} {{ $labels.ip }} mysql may has stucked query."